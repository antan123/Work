{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_from_excel(file_path):\n",
    "    df = pd.ExcelFile(file_path)\n",
    "    df = pd.read_excel(df, df.sheet_names[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy= load_file_from_excel(\"/Users/nitya/Downloads/PythonForDataScience-master/data/ID_counts1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=greedy['impressions'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "values=greedy['CTR'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=greedy['impressions'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_probs = values  # bandit probabilities of success\n",
    "N_experiments = 50  # number of experiments to perform\n",
    "N_episodes = 10000  # number of episodes per experiment\n",
    "epsilon = 0.5  # probability of random exploration (fraction)\n",
    "save_fig = True  # if false -> plot, if true save as file in same directory\n",
    "save_format = \".png\"  # \".pdf\" or \".png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    def __init__(self, bandit_probs):\n",
    "        self.N = len(bandit_probs)  # number of bandits\n",
    "        self.prob = bandit_probs  # success probabilities for each bandit\n",
    "    # Get reward (1 for success, 0 for failure)\n",
    "    def get_reward(self, action):\n",
    "        rand = np.random.random()  # [0.0,1.0)\n",
    "        reward = 1 if (rand < self.prob[action]) else 0\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(self.t) / (1 + self.counts[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "        def __init__(self, bandit,c):\n",
    "            self.c=c\n",
    "            self.k = np.zeros(bandit.N, dtype=np.int)  # number of times action was chosen\n",
    "            self.Q = np.zeros(bandit.N, dtype=np.float)  # estimated value\n",
    "        # Update Q action-value using:\n",
    "        # Q(a) <- Q(a) + 1/(k+1) * (r(a) - Q(a))\n",
    "        def update_Q(self, action, reward):\n",
    "            self.k[action] += 1  # update action counter k -> k+1\n",
    "            self.Q[action] += (1/self.k[action]) * (reward - self.Q[action])            \n",
    "        def choose_action(self, bandit):\n",
    "                exploration = np.log() /self.k\n",
    "                \n",
    "                exploration[np.isnan(exploration)] = 0\n",
    "                exploration = np.power(exploration, 1/self.c)\n",
    "                q = bandit_probs + exploration\n",
    "                action = np.argmax(q)\n",
    "                check = np.where(q == action)[0]\n",
    "                if len(check) == 0:\n",
    "                    return action\n",
    "                else:\n",
    "                    return np.random.choice(check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(agent, bandit, N_episodes):\n",
    "    action_history = []\n",
    "    reward_history = []\n",
    "    for episode in range(N_episodes):\n",
    "        # Choose action from agent (from current Q estimate)\n",
    "        action = agent.choose_action(bandit)\n",
    "        # Pick up reward from bandit for chosen action\n",
    "        reward = bandit.get_reward(action)\n",
    "        # Update Q action-value estimates\n",
    "        agent.update_Q(action, reward)\n",
    "        # Append to history\n",
    "        action_history.append(action)\n",
    "        reward_history.append(reward)\n",
    "    return (np.array(action_history), np.array(reward_history))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running multi-armed bandits with N_bandits = 1337 and agent epsilon = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nitya/anaconda2/envs/mypy/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  if sys.path[0] == '':\n",
      "/Users/nitya/anaconda2/envs/mypy/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Experiment 1/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [339 200 339 ... 385  97  76]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 2/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [696 339 163 ... 326 326 326]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 3/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [339 471 163 ... 339 271 385]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 4/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [385 339 339 ...  53 696 305]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 5/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [ 56 305 471 ... 696 149 696]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 6/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [305 163  53 ... 163 471 339]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 7/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [339 385  97 ... 385 385  53]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 8/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [ 76 326 200 ... 149 149  56]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 9/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [163 696 339 ... 163 326  76]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 10/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [696  97  97 ... 200  76 385]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 11/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [ 56 271  97 ... 163 200 149]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 12/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [696 305 200 ...  56 305  56]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 13/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [471 163  97 ... 149  76  76]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 14/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [696 200  53 ... 696 326 305]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 15/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [ 53 471 200 ... 163 305 339]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 16/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [339 696  53 ...  76 163 149]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 17/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [ 53 471  76 ...  76 696 326]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 18/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [149  76 385 ... 326 385 271]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 19/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [271 471 339 ... 385 149 305]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 20/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [163 149 339 ...  76 305 696]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 21/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [326  97 149 ... 326 305 326]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 22/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [696 305 200 ... 385 305 696]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 23/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [305  56  97 ...  97  53  97]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 24/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [271 385 271 ...  97  76  53]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 25/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [200 339 471 ... 305 305  97]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 26/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [271 385 471 ... 339 385 385]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 27/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [271  56 696 ... 163  53 305]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 28/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [326 696 385 ...  76  97 149]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 29/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [163 200 305 ... 305  76 163]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 30/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [ 76 305 163 ... 696 339 305]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 31/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [471  56  53 ...  97 149  97]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 32/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [385  56 471 ... 163 385 200]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 33/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [339 271 339 ... 385  76  76]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 34/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [ 53 339 271 ...  76 385 271]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 35/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [ 97 200 305 ... 200 385 271]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 36/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [471  76 471 ... 696 163  56]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 37/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [163  56 471 ... 271 305 163]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 38/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [200 326 149 ...  56 163 339]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 39/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [339 163 200 ...  53  56  53]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 40/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [ 56  76 385 ... 200 200 305]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 41/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [385  97 271 ...  53 385 339]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 42/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [305 149 305 ... 326 385  56]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 43/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [ 97  53 149 ...  56  76  76]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 44/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [305 471 385 ... 305 696 200]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 45/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [326 696 163 ... 696 149 271]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 46/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [326  56  56 ... 326 149 326]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 47/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [149 271 163 ...  53 471 471]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 48/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [271 149  76 ... 200 305 305]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 49/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [ 56 163 149 ...  76  53 326]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "[Experiment 50/50]\n",
      "  N_episodes = 10000\n",
      "  bandit choice history = [ 76  56 305 ... 200  53 305]\n",
      "  reward history = [0 0 0 ... 0 0 0]\n",
      "  average reward = 0.0\n",
      "\n",
      "reward history avg = [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "N_bandits = len(bandit_probs)\n",
    "print(\"Running multi-armed bandits with N_bandits = {} and agent epsilon = {}\".format(N_bandits, epsilon))\n",
    "reward_history_avg = np.zeros(N_episodes)  # reward history experiment-averaged\n",
    "action_history_sum = np.zeros((N_episodes, N_bandits))  # sum action history\n",
    "for i in range(N_experiments):\n",
    "    bandit = Bandit(bandit_probs)  # initialize bandits\n",
    "    agent = Agent(bandit, epsilon)  # initialize agent\n",
    "    (action_history, reward_history) = experiment(agent, bandit, N_episodes)  # perform experiment\n",
    "    print(\"[Experiment {}/{}]\".format(i + 1, N_experiments))\n",
    "    print(\"  N_episodes = {}\".format(N_episodes))\n",
    "    print(\"  bandit choice history = {}\".format(action_history + 1))\n",
    "    print(\"  reward history = {}\".format(reward_history))\n",
    "    print(\"  average reward = {}\".format(np.sum(reward_history) / len(reward_history)))\n",
    "    print(\"\")\n",
    "        # Sum up experiment reward (later to be divided to represent an average)\n",
    "reward_history_avg += reward_history\n",
    "        # Sum up action history\n",
    "for j, (a) in enumerate(action_history):\n",
    "    action_history_sum[j][a] += 1\n",
    "reward_history_avg /= np.float(N_experiments)\n",
    "print(\"reward history avg = {}\".format(reward_history_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_selection=action_history+1\n",
    "df = pd.DataFrame(bandit_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver(object):\n",
    "    def __init__(self, bandit):\n",
    "        \"\"\"\n",
    "        bandit (Bandit): the target bandit to solve.\n",
    "        \"\"\"\n",
    "        np.random.seed(int(time.time()))\n",
    "        self.bandit = bandit\n",
    "        n=1338\n",
    "        self.counts = [0] * self.bandit.n\n",
    "        self.actions = []  # A list of machine ids, 0 to bandit.n-1.\n",
    "        self.regret = 0.  # Cumulative regret.\n",
    "        self.regrets = [0.]  # History of cumulative regret.\n",
    "\n",
    "    def update_regret(self, i):\n",
    "        # i (int): index of the selected machine.\n",
    "        self.regret += self.bandit.best_proba - self.bandit.probas[i]\n",
    "        self.regrets.append(self.regret)\n",
    "\n",
    "    @property\n",
    "    def estimated_probas(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def run_one_step(self):\n",
    "        \"\"\"Return the machine index to take action on.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def run(self, num_steps):\n",
    "        assert self.bandit is not None\n",
    "        for _ in range(num_steps):\n",
    "            i = self.run_one_step()\n",
    "            self.counts[i] += 1\n",
    "            self.actions.append(i)\n",
    "            self.update_regret(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB1(Solver):\n",
    "    def __init__(self, bandit, init_proba=1.0):\n",
    "        super(UCB1, self).__init__(bandit)\n",
    "        self.t = 0\n",
    "        self.estimates = [init_proba] * self.bandit.n\n",
    "\n",
    "    @property\n",
    "    def estimated_probas(self):\n",
    "        return self.estimates\n",
    "\n",
    "    def run_one_step(self):\n",
    "        self.t += 1\n",
    "        i = max(range(self.bandit.n), key=lambda x: self.estimates[x] + np.sqrt(\n",
    "            2 * np.log(self.t) / (1 + self.counts[x])))\n",
    "        r = self.bandit.generate_reward(i)\n",
    "\n",
    "        self.estimates[i] += 1. / (self.counts[i] + 1) * (r - self.estimates[i])\n",
    "\n",
    "        return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-4317dd1c8b76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-147-c5236f7967c0>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, bandit)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbandit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbandit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1338\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbandit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# A list of machine ids, 0 to bandit.n-1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m  \u001b[0;31m# Cumulative regret.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "Solver(values)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
